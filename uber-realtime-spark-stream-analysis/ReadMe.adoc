:noaudio:
:scrollbar:
:data-uri:
:toc2:
:linkattrs:

= Spark Analysis of Realtime Data

.Goals
* Apache Spark Deployment and Kafka Streaming.
* Uber Realtime Data Analysis using Advanced Concept of Spark (SparkSQL)
* Usage of Interactive NoteBooks like Apache Zeppelin


.Prerequisite
* Skills
** Don't clean up the Lab1 Environment, As we are going to use the Kafka Cluster to do the Realtime data streaming
** Programming Knowledge of Python, Scala and Java Languages
** Knowledge of GIT Commands
** Knowledge on OpenShift Deployments
** Knowledge of Messaging system and Messaging Architecture
** Basic Knowledge of Cluster Computing and Distributed Architetcures
** GIT Clone of the Source code into Student's Desktop

* Tools
** `curl` utility
** `sed` utility
** `oc` version 3.9 utility
** `Interactive Notebooks - Jupyter and Zeppelin`
** `git` command basics

 
:numbered:

== Overview

We will build a real-time example for analysis and monitoring of Uber car GPS trip data. 

The first project discussed creating a machine learning model using Apache Sparkâ€™s K-means algorithm to cluster Uber data based on location. This complex project will discuss using the saved K-means model with streaming data to do real-time analysis of where and when Uber cars are clustered

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/picture1.png[uberstream10]

=== Background


=== Reference

== Lab Asset Overview

=== Environment Variables

=== OpenShift access

=== Code Analysis of Lab 3

https://github.com/gpe-mw-training/operational_intelligence/tree/master/uber-realtime-spark-streaming/

There are 6 main programs in the GitHub, We can individually deploy as a Spark Job or Can execute in the Zeppelin Notebook.
Entire Flow is with KafkaProducerConsumer.scala. Hence we analyse with this code and apply in the zeppelin notebook.

=== Data Pipeline

A Spark streaming application subscribed to the first topic:

==== Ingests a stream of uber trip events

==== Identifies the location cluster corresponding to the latitude and longitude of the uber trip.

==== Adds the cluster location to the event and publishes the results in JSON format to another topic.

A Spark streaming application subscribed to the second topic:

==== Analyzes the uber trip location clusters that are popular by date and time.

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/picture2.png[uberstream11]

=== Data Flow in Detail
The example data set is Uber trip data, which you can read more about in part 1 of this series. The incoming data is in CSV format, an example is shown below , with the header:

date/time, latitude,longitude,base
2014-08-01 00:00:00,40.729,-73.9422,B02598

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/picture3.png[uber12]

Data will be enriched are in JSON Format which is given below

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/picture4.png[berstream13]

Spark Kafka Producer Consumer Code with Enriched Data

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/picture5.png[berstream14]

=== Perform these steps on the code or in Zeppelin Notebook

Parse the DataSet Records (Uber case class)

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update1.png[berstream14]

Load the KMeans model

The Spark KMeansModel class is used to load the saved K-means model fitted on the historical Uber trip data.

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update2.png[berstream14]

Output of model clusterCenters:

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update3.png[berstream14]


Below the cluster centers are displayed on a google map:


image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update4.png[berstream14]



=== Spark Streaming Code

These are the basic steps for the Spark Streaming Consumer Producer code:

==== Configure Kafka Consumer Producer properties.

The first step is to set the KafkaConsumer and KafkaProducer configuration properties, which will be used later to create a DStream for receiving/sending messages to topics. You need to set the following paramters:

Key and value deserializers: for deserializing the message.
Auto offset reset: to start reading from the earliest or latest message.
Bootstrap servers: this can be set to a dummy host:port since the broker address is not actually used by Spark Streams.


==== Initialize a Spark StreamingContext object. Using this context, create a DStream which reads message from a Topic.

ConsumerStrategies.Subscribe, as shown below, is used to set the topics and Kafka configuration parameters. We use the KafkaUtils createDirectStream method with a StreamingContext, the consumer and location strategies, to create an input stream from a MapR Streams topic. This creates a DStream that represents the stream of incoming data, where each message is a key value pair. We use the DStream map transformation to create a DStream with the message values.

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update5.png[berstream14]


image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update6.png[berstream14]


Apply transformations (which create new DStreams).

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update7.png[berstream14]


image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update8.png[berstream14]

A VectorAssembler is used to transform and return a new DataFrame with the latitude and longitude feature columns in a vector column.


image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update9.png[berstream14]



Then the model is used to get the clusters from the features with the model transform method, which returns a DataFrame with the cluster predictions.



image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update10.png[berstream14]


Write messages from the transformed DStream to a Topic.

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update11.png[berstream14]



The Dataset result of the query is converted to JSON RDD Strings, then the RDD sendToKafka method is used to send the JSON key-value messages to a topic (the key is null in this case).

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update12.png[berstream14]


image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update13.png[berstream14]



Start receiving data and processing. Wait for the processing to be stopped.
==== To start receiving data, we must explicitly call start() on the StreamingContext, then call awaitTermination to wait for the streaming computation to finish.



Spark Kafka Consumer Code

Next, we will go over some of the Spark streaming code which consumes the JSON-enriched messages.

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update14.png[berstream14]


We specify the schema with a Spark Structype:

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update15.png[berstream14]

Below is the code for:

Creating a Direct Kafka Stream
Converting the JSON message values to Dataset[Row] using spark.read.json with the schema
Creating two temporary views for subsequent SQL queries
Using ssc.remember to cache data for queries

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update16.png[berstream14]


Now we can query the streaming data to ask questions like: which hours had the highest number of pickups? (Output is shown in a Zeppelin notebook):


spark.sql("SELECT hour(uber.dt) as hr,count(cluster) as ct FROM uber group By hour(uber.dt)")



image::https://github.com/Pkrish15/uber-datanalysis/blob/master/update17.png[berstream14]


How many pickups occurred in each cluster?

df.groupBy("cluster").count().show()

or

spark.sql("select cluster, count(cluster) as count from uber group by cluster")



Which hours of the day and which cluster had the highest number of pickups?

spark.sql("SELECT hour(uber.dt) as hr,count(cluster) as ct FROM uber group By hour(uber.dt)")



Display datetime and cluster counts for Uber trips:

%sql select cluster, dt, count(cluster) as count from uber group by dt, cluster order by dt, cluster





We will go through each of these steps with the example application code.

=== Configure Spark Kafka Consumer Producer Properties

The first step is to set the KafkaConsumer and KafkaProducer configuration properties, which will be used later to create a DStream for receiving/sending messages to topics. You need to set the following paramters:


Key and value deserializers: for deserializing the message.

Auto offset reset: to start reading from the earliest or latest message.

Bootstrap servers: this can be set to a dummy host:port since the broker address is Strimzi Kafka POD


----
...

[root@localhost ~]# oc login -u user5 -p r3dh4t1! https://master.6d13.openshift.opentlc.com/
Login successful.

You have one project on this server: "uber-realtimedata-analysis-user5"

Using project "uber-data-user5".
[root@localhost ~]# oc get routes
NAME              HOST/PORT                                                         PATH      SERVICES          PORT       TERMINATION   WILDCARD
apache-zeppelin   apache-zeppelin-uber-realtimedata-analysis-user5.apps.6d13.openshift.opentlc.com             apache-zeppelin   8080-tcp                 None
...
----

== Zeppelin Configuration Changes to run the code

This is one of the most critical steps, Please follow the screen shot's carefully. Missing a single step will lead to unexpected results and exceptions.

----
...
$ oc get pods

$ oc rsh apache-zeppelin-2-dr8s6

sh-4.2$ cd /opt/zeppelin/conf/

sh-4.2$ ls
configuration.xsl  log4j_yarn_cluster.properties  zeppelin-site.xml
interpreter-list   shiro.ini.template		  zeppelin-site.xml.template
interpreter.json   zeppelin-env.cmd.template
log4j.properties   zeppelin-env.sh.template

sh-4.2$ mv zeppelin-env.sh template zeppelin-env.sh

sh-4.2$ vi zeppelin-env.sh

export SPARK_SUBMIT_OPTIONS="--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0"

sh-4.2$ esc+wq!

...
----
== Zeppelin UI Changes to Run the Code

Make Changes in Spark.Memory Parameters to 5G

zeppelin Dependency Local Repo as shown in the Figure

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/ZeppelinUIChangesLab3.png[zepp5ui]


== Conclusions

Finally you have learned the concepts of Spark Cluster, Actions, Transformations, Spark SQL and NoteBook Deployment.


== Questions

TO-DO :  questions to test student knowledge of the concepts / learning objectives of this lab

== Appendix

===  Overview 

So far we learned about Spark uses Zeppelin Notebook and Performs the Data Analysis based on the Uber RealTime Data.


ifdef::showscript[]

endif::showscript[]
